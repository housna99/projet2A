<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="css/style.css" />
    <title>Projet 2A</title>
  </head>
  <body>
    <header>
      <div class="logo">Mines Nancy</div>
      <div class="toggle"></div>
      <div class="navigation">
        <ul>
          <li><a href="index.html">Sujet</a></li>
          <li><a href="contact.html">Contact</a></li>
        </ul>
        <div class="social-bar" >
          <ul>
            <li>
              <a href="https://www.depinfonancy.net/">
                <img src="images/web-ecole-des-mines-de-nancy.png" target="_blank" alt="" style="width: 270%;"/>
              </a>
            </li>
            <li>
              <a href="https://github.com/nathancirca/database_training" >
                <img src="images/github.png" target="_blank" alt="" style="width: 150%; margin-left:30px;"/>
              </a>
            </li>
            <li>
              <a href="https://instagram.com">
                <img src="images/instagram.png" target="_blank" alt="" />
              </a>
            </li>
          </ul>
          <a href="mailto:you@email.com" class="email-icon">
            <img src="images/email.png" alt="" />
          </a>
        </div>
      </div>
    </header>

    <section class="home">
      <img src="images/job-sharing-pair.png" class="home-img" alt="" />
      <div class="home-content">
        <h1>
          We help to achieve <br />
          your goals.
        </h1>
        <p>
          Ce projet a été réalisé dans le cadre du projet de deuxième année du département informatique au sein de l'école des Mines de Nancy. <br/>

          C'est le fruit d'un travail qui a été encadré par Monsieur Briot Loick et fait par CHANA Housna et GALMICHE Nathan.
        </p>
        <a href=https://github.com/nathancirca/database_training class="btn">code source</a>
      </div>
    </section>
    <section class="desc_projet">
      <div class="home-content">
        <h3>Présentation de sujet :</h3>
        <p>
            La labellisation (ou annotation) est un travail que doit réaliser de plus en plus d'entreprises, 
            cette étape étant primordiale à l'utilisation d'apprentissage automatique supervisée.
            <br/>
            Les données publiques des grandes banques de données n'étant pas toujours adaptées ou existantes, 
            il est nécessaire de pouvoir se créer ses propre jeux de données.
            <br/>
            Dans le cadre du projet SCAR, cette thématique est très présente notamment pour entraîner des modèles d'IA 
            permettant de reconnaître des objets, effectuer de la squeletisation... etc 
            <br/>
            <br/>
            L'idée est de continuer le travail déjà réalisé sur la plateforme d'annotation pour la rendre plus efficace,
            plus facilement utilisable et développer de nouvelles fonctionnalités (export de données dans différents formats, 
            annotation de vidéo, annotation d'image, 
            annotation avec des formes polygonales et non rectangulaire comme souvent réalisé... etc)
            <br/>
        </p>
        <!-- <a href="contact.html" class="btn">Get Started</a> -->
      </div>
    </section>
    <section class="etape_projet">
        <div class="services">
            <div class="service">
              <div class="icon1">
                <img src="images/001.png" alt="" />
              </div>
              <h2>Phase de recherche</h2>
              Lorem ipsum, dolor sit amet consectetur adipisicing elit. Harum omnis
              nemo sapiente in quidem sed dolores cumque! Ut, est aliquid!
            </div>
            <div class="service">
              <div class="icon">
                <img src="images/002.png" alt="" />
              </div>
              <h2>Phase d'initiation aux bases d'utilisation du framewok OpenCV</h2>
              Lorem ipsum, dolor sit amet consectetur adipisicing elit. Harum omnis
              nemo sapiente in quidem sed dolores cumque! Ut, est aliquid!
            </div>
            <div class="service">
              <div class="icon">
                <img src="images/003.png" alt="" />
              </div>
              <h2>Phase d'entrainement du modèle Yolov5 sur la dataset du robot SCAR</h2>
              Lorem ipsum, dolor sit amet consectetur adipisicing elit. Harum omnis
              nemo sapiente in quidem sed dolores cumque! Ut, est aliquid!
            </div>
            <div class="service">
              <div class="icon">
                <img src="images/004.png" alt="" />
              </div>
              <h2>Phase de création de notre propre dataset</h2>
              Lorem ipsum, dolor sit amet consectetur adipisicing elit. Harum omnis
              nemo sapiente in quidem sed dolores cumque! Ut, est aliquid!
            </div>
            <div class="service">
              <div class="icon">
                <img src="images/005.png" alt="" />
              </div>
              <h2>Phase de l'export en format YOLO et de la résolution des problèmes des classes</h2>
              Lorem ipsum, dolor sit amet consectetur adipisicing elit. Harum omnis
              nemo sapiente in quidem sed dolores cumque! Ut, est aliquid!
            </div>
            <div class="service">
              <div class="icon">
                <img src="images/006.png" alt="" />
              </div>
              <h2>Phase d'entrainement du modèle Yolov5 sur notre dataset</h2>
              Lorem ipsum, dolor sit amet consectetur adipisicing elit. Harum omnis
              nemo sapiente in quidem sed dolores cumque! Ut, est aliquid!
            </div>
          </div>
    </section>
    <br/>
    <br/>
    <br/>
    <br/>
      <section>
        <div class="home-content2">
          <div class="recherche">
            <h1>Phase de recherche : </h1>
            <br>
            <ul class="q_recherche">
                <li> <h3> Quelles sont les différentes bases de données qui existent? Quel sont leurs formats d'annotation?</h3>
                </li>
                <li><h3> Quelles sont les différents types d'annotation?</h3></li>
            </ul>
            <br>
            <h3>Les bases de données trouvées : </h3>
            <p>Nous avons remarqué qu'il existe bien evidement un gand nombre de datasets contenant un volume important de contenu.
                Nous avons choisi de s'intéresser à quelques unes.</p>
                <!-- <ul class="datasets_list"> -->
                    <div><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit"><h4>Pascal VOC</h4></a> </div>
                  <div>
                    <ul class="pascalvoc">
                       <li> Elle fournit des images standardisées pour la reconnaissance d'objets. </li>
                    <li>Elle contient 11 530 images et comporte 20 classes.</li>
                    <li> Format de données de Pascal VOC est : un fichier XML</li>
                      <li>Le Bounding Box, qui est pour rappel une méthode d'annotation, a comme coordonnées: xmin-haut gauche, ymin-haut gauche,xmax-bas droite, ymax-bas droite</li>
                    </ul>
                </div>
              <div><a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit"><h4>COCO</h4></a> </div>
              <div>
                  <ul class="coco">
                     <li> COCO est constitué d'images à grande échelle avec des objets communs en contexte pour la détection d'objets, la segmentation et l'ensemble de données de sous-titrage. 
                         </li>
                  <li>COCO dispose de 1,5 million d'instances d'objets pour 80 catégories d'objets </li>

                  <li> COCO stocke les annotations dans un fichier JSON
                    </li>
                    <li>Le Bounding Box, a comme coordonnées:  x-haut gauche, y-haut gauche, largeur, hauteur</li>
                  </ul>
              </div>
              <div>
                <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit"><h4>imageNET</h4></a> 
              </div>
              <div>
                  <ul class="imageNet">
                        <li> COCO est constitué d'images à grande échelle avec des objets communs en contexte pour la détection d'objets, la segmentation et l'ensemble de données de sous-titrage. </li>
                      <li>COCO dispose de 1,5 million d'instances d'objets pour 80 catégories d'objets </li>

                      <li> COCO stocke les annotations dans un fichier JSON
                        </li>
                        <li>Le Bounding Box, a comme coordonnées:  x-haut gauche, y-haut gauche, largeur, hauteur</li>
                  </ul>
              </div>
             <div>
                <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit"><h4>INRIA</h4></a> 
              </div>
              <div>
                  <ul class="inria">
                          <li> COCO est constitué d'images à grande échelle avec des objets communs en contexte pour la détection d'objets, la segmentation et l'ensemble de données de sous-titrage. 
                              </li>
                        <li>COCO dispose de 1,5 million d'instances d'objets pour 80 catégories d'objets </li>

                        <li> COCO stocke les annotations dans un fichier JSON
                          </li>
                          <li>Le Bounding Box, a comme coordonnées:  x-haut gauche, y-haut gauche, largeur, hauteur</li>
                  </ul>
              </div>
              <br>
            <h3>Les types d'annotations : </h3>
            <p>
                Déja, c'est quoi <b>annoter</b> des données ? </p> 
                
                <p> Il est vrai qu'un ordinateur est capable de faire des calculs complexes dans un laps de temps très court.
                Pourtant, cette même machine serait incapable de distinguer entre un chien et un chat sur une photo. 
                Cette tâche peut sembler facile pour nous, car cette capacité est presque innée pour un humain.
                Pour y arriver, il faut faire appel à des algorithmes d'entrainement à partir d’un ensemble de données. 
                Et ce sont ces données qui doivent être étiquetées afin de présenter la "cible" que l’on souhaite que le modèle de Machine Learning apprenne à prédire.
            </p>
            <p>
              Il existe de nombreux types d’annotations, en fonction des tâches que nous souhaitons effectuer. Parmi les exemples, on peut citer les polygones, les points de repère, la 2D, la 3D, la boîte de délimitation, le masquage, le suivi, la polyligne, etc
            </p>
              <p>
                  Nous avons choisi de s'interesser aux Bounding boxes (boites englobantes) puisque notre projet repose en gros sur cette technique d'annotation.
                  En effet, ce type d’annotation aide à faire des prédictions dans la vie réelle et à reconnaître les objets avec précision.
              </p>
              <p>
                <i>(IMAGE exemple) </i>
            </p>
          </div> 
        </div> 
        <div class="home-content2">
          <div class="openCV">
            <h1>Phase d'initiation aux framework openCV' : </h1>
            <p>

              Lorem ipsum dolor sit amet. Est recusandae aspernatur et voluptatem dolorum et veritatis dolores sit repellat aspernatur! Ut ipsa quisquam et accusantium possimus aut debitis sunt id harum consequatur sed animi maiores ut consequatur tempora a reprehenderit fuga! Sit autem atque qui amet illo et dolorem accusamus est pariatur beatae ad animi quia qui galisum doloribus aut illo repudiandae. In quisquam totam et velit quae et dicta exercitationem et perferendis cumque rem assumenda quis est enim neque aut repellendus dolores.
              
              Et ducimus omnis qui itaque autem hic officia voluptatibus ut assumenda laudantium. Aut tempore minus non consequatur voluptas et temporibus omnis et voluptas quasi et quia impedit sit animi distinctio aut repudiandae necessitatibus? Aut maiores voluptatem rem autem voluptatem eos recusandae molestiae ea internos enim ut sunt quod numquam recusandae. Sit maiores mollitia id dolore debitis id asperiores ipsam!
              
              Qui aliquid autem cum nobis voluptas est deserunt quia. Non debitis harum eos doloremque voluptatem qui rerum odio et vitae recusandae aut sunt labore?
              </p>
         
          </div> 
        </div>
        <br>
        <div class="home-content2"> 
            <div class="entrainement1">
              <h1>Phase d'entrainement 1 : </h1>
              <br>
              <h2>C'est quoi YOLOv5 ?</h2>
              <p>
                YOLO, acronyme de 'You only look once', 
                est un algorithme de détection d'objet qui divise les images en un système de grille. 
                Chaque cellule de la grille est responsable de la détection des objets en elle-même. 
                YOLOv5 représente la version la plus récente de l'algorithme YOLO, et c'est en se servant de lui que nous allons entrainer notre propre dataset.
              </p>
              
              <br/>
              <h2>Préparation de notre première dataset </h2>
              <p>Pour le moment, n'ayant pas encore créé notre base de données, on se servira de la base de donnée du robot SCAR que nous avons nommée "Spot_recognition" et qui contient 1017 images.

                En gros,nous avons regroupé des 
                images du robot SCAR et bien évidement
                 nous avons ajouté d'autres images qui 
                 ne contiennent pas le robot pour éviter le problème de surapprentissage </p>
                <br/>
              <p>
                <i>(Image de la dataset) </i>
                <h2>
                  c'est temps d'entrainer !
                </h2>
                <p>
                  Afin d'entrainer notre dataset , nous avons utilisé la plateforme Roboflow qui nous a permit d'annoter toutes nos images et puis de diviser notre dataset en une dataset d'entrainement "train" et une autre pour le "test" .
                  Ensuite, nous avons suivi les commandes suivantes : 
                  <br/>
                  <br/>
                  <br/>
                  On commence par cloner le YOLOV5 repository sur github : <br/>
                  <h4>git clone https://github.com/ultralytics/yolov5.git</h4>
                  <br/>
                  Ensuite, on entraine notre base de données en lançant le script "train.py" : <br/> 
                  <h4>python3 train.py --weights yolov5s.pt --data ../Spot_recognition/data.yaml --epochs 10 --batch 1 </h4> <br/>
                  A titre d'information , le batch size correspond au nombre d'échantillons traités avant la mise à jour du modèle.
                  Le nombre d'époques correspond au nombre de passages complets dans l'ensemble de données d'apprentissage.
                  <br>
                  Ici, nous avons sélectionné YOLOv5s, le modèle le plus petit et le plus rapide disponible. 
                  Pour rappel , il existe d'autres modèles : <br/></p>
                <a href="https://github.com/ultralytics/yolov5/releases">Pour plus d'informations</a><br/>
                  <img src="images/yolov5s.png" target="_blank" alt="" style="width: 100%;"/>
                  <br/>
                  En se servant d'une caméra, et après avoir lancer la commande suivante , on s'assure si notre modèle arrive a différencier entre un robot scar et un autre objet. <br/>

                  <h4>python3 detect.py --weights runs/train/exp6/weights/best.pt --source 0 </h4>
                  Le exp6 est un fichier qui a été créé suite à l'entrainement du modèle. La source 0 veut simplement dire qu'on a utilisé une caméra.
                  <br/>
                  <p>Dans notre cas,  notre modèle arrivait à reconnaitre le robot scar, par contre parfois il ne distinguait pas entre une chaise et le robot ou entre un objet coloré en jaune et le robot!   </p>
                  <h4>python3 val.py --weights runs/train/exp6/weights/best.pt --data ../dataset_waffle/data.yaml --task test </h4>
                  <i>(Clarificication )</i>
                </p>
                <p>Les résultats n'étaient pas satisfaisantes, du coup nous avons :
                  <ul>
                    <li>Ajouter plus de données à la base Spot_recognition</li>
                    <li>Annoter les nouvelles images avec Roboflow</li>
                    <li>Réentrainer le modèle Yolov5</li>
                  </ul> 
                </p>
                <p>Conclusion : Notre deuxième tentative était meilleure que la première; et c'est là ou on a vraiment compris l'importance d'avoir plus de données pour entrainer un modèle!</p>
            </div>
            <div class="home-content2"> 
              <div class="bdd">
                <h1>Phase de création de notre propre jeu de données : </h1>
                Ayant compris l'importance d'un grand volume de données; nous avons décidé de créer une base de donnée sous SQLite qui regroupera des images issues des datasets suivantes :
                <ul>
                  <li>PASCAL VOC</li>
                  <li>COCO</li>
                  <li>Dataset waffles (déja crééé)</li>
                  <li>Spot_recognition</li>
                  <li>Dataset running</li>
                </ul>
                <h1>Architecture de la base de donnée :</h1>
                <p>
                Pour rappel, l'objectif est de créer une base de données qu'on pourra l'utiliser quelque soit le format d'annotation. <br>
                Avant de remplir notre base, il fallait alors penser à une architecture qui va nous permettre à réaliser notre objectif.

        
                </p>
                <p>L'idée était alors de concevoir une base de donnée sous SQlite ayant 3 tables : 
                  <ul class="réentrainement">
                    <li>une table Images; qui contiendera nos images avec leurs chemins </li>
                    <li>une table Objets; qui contient chaque objet dans une image (selon le fichier d'annotations) et un numéro de classe qu'on va lui attribuer </li>
                    <li>une table Lignes; qui contiendera les points x1,y1,x2 et y2 .C'est-à-dire les points du bounding box (xmin-haut gauche, ymin-haut gauche,xmax-bas droite, ymax-bas droite) </li>
                  </ul>
                </p>
                <i>(SCreen dataset)</i>


      </section>


    <script src="js/script.js"></script>
  </body>
</html>